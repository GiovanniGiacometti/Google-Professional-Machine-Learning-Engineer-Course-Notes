
After some training, if the loss of the training dataset and the validation dataset are similar, it means we can keep training (we have not overfitted)

---

VertexAI does experiments all together, more efficiently

---

We can't use the validation error as representative, as we used that dataset to choose the architecture.
We need another dataset, the test dataset.
If the model fails with it, we should collect more data.

Another option is to split the training - validation split multiple times, average all losses (so that also std is available) -> cross-validation

Pro: use all data, good especially if there few samples
Con: takes more time



