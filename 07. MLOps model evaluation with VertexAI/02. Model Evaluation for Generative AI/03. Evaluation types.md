
- Binary evaluation: easy to implement, lack feedback
- Categorical evaluation: more categories, increased complexity (positive/neutral/negative sentiment analysis)
- Ranking: assess relative quality of outputs - can be resource extensive
- Numerical Evaluation (blue, rouge, f1) - might not capture all nuances
- Text Evaluation, like domain expert evaluation, can be time consuming or subjective
- Multi-task Evaluation: multiple judging types, both qualitative and quantitative - resource and time consuming (like evaluating on summarization and translation using metrics and human reviews)

---

- Lexical similarity: measure similarity between model's output and a desired text (both semantically and words) - examples are bleu, rouge, meteor
- Linguistic quality: focuses on quality of the generated text (bleurt, human evaluation, perplexity)
- Task specific metrics: specific to the assignment (like question answering)
- Safety and fairness
- Groundedness: fact checking, human evaluation
- User-centric Metrics: do users find the output satisfying?

Some metrics fit into more categories

---

Diversity metrics measure the diversity with the goal of avoiding repeating always the same concepts

- Distinct-N
- Entropy
- Self-BLUE
- Mauve (compares distribution of words with a reference)
- Coverage (of a reference dataset)

