

**Source**: where the pipeline gets the input data

**Steps**: each step is a transform

**Sink**: where the result goes, the output

**Runner**: executes the code of the pipeline. They are platform specific (there is one for dataflow, one for spark...)

**PCollection**: the input and output of each step. They don't store all data in memory, it's like a pointer to where data is actually stored

---

Many connectors, such as Big Query.

Both for input and output.

---

There could be multiple servers trying to execute a step, for instance the final step. To prevent concurrency issues, the output is automatically sharded