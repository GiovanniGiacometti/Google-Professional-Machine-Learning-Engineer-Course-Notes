
https://cloud.google.com/vertex-ai/docs/training/configure-compute#gpu-compatibility-table

---

CPU: performs operation serially, inefficient when many computations are needed

GPU: helps the CPU and applies some operation in parallel, making it faster

---

When devising a machine for training, make sure the required GPUs are available in the region when custom training is done.

From the doc (reading there as the book is a bit out of date):

- Configuration details are to be specified withing a WorkerPoolSpec

In there, you specify the machine type. Each replica in the worker pool runs on a separate VM that has the specified machine type.

If the code uses GPUs, you can configure the worker pool to use one or more GPUs on each VM

- To use GPUs, you must use an A2, N1, or G2 machine type

- Vertex AI supports the following types of GPU for custom training:
	
	- `NVIDIA_H100_80GB`
	- `NVIDIA_A100_80GB`
	- `NVIDIA_TESLA_A100`Â (NVIDIA A100 40GB)
	- `NVIDIA_TESLA_P4`
	- `NVIDIA_TESLA_P100`
	- `NVIDIA_TESLA_T4`
	- `NVIDIA_TESLA_V100`
	- `NVIDIA_L4`

- The type of GPU specified must be available in the location where custom training is performed

- Only a certain number of GPUs is allowed (like 2 or 4 NVIDIA_TESLA_T4, not three)

- You must make sure the the GPU configuration provides sufficient virtual CPUs and memory to the machine type (not so clear the example they make, why does the GPU need to "provide" virtual cpus??)



### TPU

Specialized hardware designed for ML workloads

Multiple matrix multiply units

They can be connected in Pods

Not suited for workloads that require frequent branching, sparse data and also high precision, custom operations


