
Define a schema to have consistent and reproducible checks.

It's the output of the data analysis process.

It describes the property of your data to allow to easily catch anomalies

---

Big Data -> multiple machines to scale data validation for large datasets

**TensorFlow Data Validation** can be used to understand and validate and monitor data at scale.

It is part of the [[TFX]] platform and provides libraries for data validation and schema validation for large datasets.

The library is used in 2 contexts:

- Exploratory Data Analysis
- Production Pipeline phase -> after the model is deployed, the schema produced before can be used to detect changes

From the book:
>Its API are built on the Apache Beam sdk for building batch and streaming pipelines.
>[[Dataflow]] is a managed service that runs apache beam data processing pipelines at scale

From the repo:

>Apache Beam is required; it's the way that efficient distributed computation is supported. By default, it runs in local mode but can also run in distributed mode using Google Cloud [[Dataflow]] and other Apache Beam runners


