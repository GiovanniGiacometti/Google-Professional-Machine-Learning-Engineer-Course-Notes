

Imbalanced dataset: 
- oversampling -> duplicating samples from minority class
- undersampling -> deleting samples from majority class

Another way to handle imbalanced dataset is to downsample and upweight the majority class
How to oversample? Assign sample weights

---

In google website, they suggest to try training the model on the original dataset (especially if the imbalance is not too high). At least this model provides a good baseline for future experiments.

In general, imbalance isn't always a problem

---

How to split data in train, test, validation?

If random, we might introduce a skew -> consider stratification

For online systems, it's recommended to split the data by time (since training data are older than serving data)


---

https://cloud.google.com/sensitive-data-protection/docs/automating-classification-of-data-uploaded-to-cloud-storage#building_the_quarantine_and_classification_pipeline

---

auto ml single time column?

---

You are training a Resnet model on AI Platform using TPUs to visually categorize types of defects in automobile engines. You capture the training profile using the Cloud TPU profiler plugin and observe that it is highly input-bound. You want to reduce the bottleneck and speed up your model training process. Which modifications should you make to the tf.data dataset? (Choose two.)