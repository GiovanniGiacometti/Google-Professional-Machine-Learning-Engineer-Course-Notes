
When deploying a model using a vertex ai prediction endpoint, the autoscaling for the container needs to be specified. It provisions the container resources and sets up autoscaling

More than one model can be deployed in an endpoint, one model can be deployed to more than on endpoint

2 ways:

- API

Provide a traffic-split -> percent of the traffic at the endpoint that goes to this model

machine-type -> type of machine used

- Using console 

Go to [[Vertex AI Model Registry]]

---

You get a REST endpoint ID that you can embed in a website or API gateway or App Engine to create an ML gateway for real-time prediction.


### A/B Testing of different versions

Add the new model serving a small percentage of traffic and gradually increase the new one if performing better

### Batch Predictions

Point to the model and the input data to run a batch prediction job

Many formats available (jsonl, tfrecord in cloud storage, csv...)