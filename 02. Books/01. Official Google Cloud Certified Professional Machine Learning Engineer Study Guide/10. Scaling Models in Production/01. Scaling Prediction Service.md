
After a TF model is trained, you get a SavedModel, which contains everything, meaning that it doesn't need the code to run again
It can be used for sharing

### TF Serving

Allows to host a trained TF model as an API endpoint through a model server

2 types of endpoints

REST and gRPC


using Docker is recommended (there are managed tf prebuilt containers)

### Serving a Saved Model with TF Serving

![[Pasted image 20241010232859.png]]

To know how the response format of the predict will look like, you can look at the SavedModel0s SignatureDef -> there is a cli to inspect

![[Pasted image 20241010233104.png]]

Here an example

both class id and classes have the same shape (-1, 1) -> only one value

Same for logits and probabilities