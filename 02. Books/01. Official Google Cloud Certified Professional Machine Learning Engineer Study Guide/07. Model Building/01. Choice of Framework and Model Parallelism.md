
### Data Parallelism

Dataset is split into parts and assigned to parallel computational machines. Same parameters are used.
Then, gradient is computed and sent back to the main node, where model update is performed

Can be:

- synchronous training -> each gpu has a complete copy of the model and trains it on its part of data -> all reduce algorithm is used to collect parameters

- async training -> each worker updates parameter independently and asynchronously

An example is parameter serving strategy in TF

### Model Parallelism

Every model is partitioned into parts

It can be used to train a model that doesn't fit in a single GPU

Notice that GPU2 waits for GPU1 to complete the training pass

---

Strategies in TF:

- mirrored strategy -> sync training on multiple gpus on one machine
- multi worker mirrored -> sync training across multiple workers, each with a gpu or more
- central storage strategy -> sync training, no mirroring
- tpu strategy -> sync training on multiple cpus
- parameter serving -> some machines are worker and others parameter servers
