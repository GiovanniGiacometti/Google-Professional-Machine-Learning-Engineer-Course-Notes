
### Offline or batch predictions

Data received in batches

Vertex AI batch prediction can be used

![[Pasted image 20241007235506.png]]

### Online predictions

Real time

Can be:

- synchronous -> the caller waits -> Vertex AI online predictions can be used to deploy the model. App Engine might be used to perform some feature preprocessing

![[Pasted image 20241007235625.png]]

- asynchronous -> the user might not query the model directly. It may:
	- push: the model generates predictions and send them to the user
	- poll: the user periodically polls the database where predictions are stored
![[Pasted image 20241007235752.png]]
How to minimize latency?

- model level: smaller model, accelerators
- serving level: lookup data store, precompute predictions, caching