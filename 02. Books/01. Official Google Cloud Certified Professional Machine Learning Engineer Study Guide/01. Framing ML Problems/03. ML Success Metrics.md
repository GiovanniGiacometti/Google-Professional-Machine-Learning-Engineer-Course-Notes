
Recall = TP / (TP + FN)

Note it is also the true positive rate

Higher if FN are low -> Higher if we predict correctly most of the data points

Low false negatives -> ideal in cases where finding the positive class is crucial (like medical)

Focus on true positive cases

---

Precision = TP / (TP + FP)

Higher if FP are low -> Higher if we correctly classify positive points

Ideal if we want to minimize the false positives

Focus on predicted positives: it quantifies the percentage of positive predictions that were actually correct

---

F1 = 2 * Precision * Recall / (Precision + Recall)

Armonic mean of precision and recall.

It aims to lower false negative and false positive together

---

ROC:

False positive rate ( FP/(FP + TN) ) on x axis

True positive rate (recall) on y axis 

On point for each classification threshold

A way to compare 2 models is the area under curve (AUC)

- it is scale invariant 
- classification threshold invariant (not always desirable, sometimes there are huge disparities between false positives and false negatives)

---

AUC PR: area under precision recall curve

Preferred when dataset is highly imbalanced (the AUC curve can be skewed because of a high number of true negatives)



