
- Exploding gradients: batch normalization and lower learning rate can help (also clipping)

- dead relu unit: lower the learning rate

- vanishing gradients: relu can help
