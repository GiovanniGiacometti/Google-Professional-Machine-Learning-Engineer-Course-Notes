
Penalty term that represents a magnitude of bias

2 methods:

- MinDiff

Focuses on the difference in distributions of the prediction of the models in different subsets ->
goal is to achieve equality of opportunity

![[Pasted image 20240928004100.png]]

- Counterfactual logic pairing

![[Pasted image 20240928004216.png]]

The score is different when the identity token changes

Minimize loss that includes a penalty for the difference in scores when a sensitive feature has a counterfactual example

Address flip rate (probability that a classifier changes prediction if the identity attribute in a given feature was changed)


MinDiff -> achieve equality of opportunity
CLP -> counterfactual fairness