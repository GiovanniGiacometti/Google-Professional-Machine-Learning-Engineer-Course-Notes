
Bias: stereotyping, prejudice, favoritism towards some things, people or groups over others.

  
Although not all biases are harmful, it is important to note that we naturally obtain some tendencies in perceptions, thought processes, and common senses, depending on a group you belong to or information you get.

More than 100 hundreds types of bias. They can show themselves in models

Most commong:

- Reporting: frequency of events in a dataset doesn't reflect the actual real world frequency (people tend to report unusual things)

- Automation: tendency to favor results generated by automated systems over those generated by non automated systems

- Selection: dataset examples are chosen in a way that is not reflective of their real world distribution

Can take many forms:
- coverage (data is not selected in a representative fashion)
- non-response (some groups do not response)
- sampling (not proper sampling techniques)

- Group attribution bias is a tendency to generalize what is true of individuals to an entire group to which they belong. It can be:
	- in group -> preference for members of a group to which you also belong, or for characteristics that you also share.
	- out group ->Â tendency to stereotype individual members of a group to which you do not belong

- Implicit bias occurs when assumptions are made based on one's own mental models and personal experiences that do not necessarily apply more generally,

A common form of implicit bias is confirmation bias, where model builders unconsciously process data in ways that affirm pre-existing beliefs and hypotheses.

A model builder might actually keep training a model until it produces a result that aligns with their original hypothesis, this is called experimenters bias.

---

What about AI?

Bias can enter in the system as a systematic error in many steps:

- data collection (selection bias)
- training (feeding biased dataset -> the model finds and repeats the bias patterns and predictions)
- Bias can even appear after deployment from feedback loops and model iterations.

You can rarely identify a single cause of or a single solution to these bias problems -> range of solutions needed

---


Not all types of bias are necessarily unfair
Definion of fairness:

Decisions made by computers after a machine-learning process might be considered unfair if they were based on variables considered sensitive (gender, sex, race...)

---

AI systems have the potential to be fairer and more inclusive at a broader scale

Risk: negative wide scale impact

---


AI fairness is actually a difficult task to achieve:

- AI models learn from existing data, and an accurate model might learn or even amplify problematic preexisting biases.
- Even with the most rigorous and cross-functional training and testing, it is a challenge to build systems that will be fair across all situations.
- no standard definition of fairness, whether decisions are made by humans or machines.
- Identifying appropriate fairness criteria for a system requires accounting for considerations such as user experience, cultural, social, historical, political, legal, and ethical. People may disagree about what is fair, and it may be unclear what point of view should dictate policy, especially in a global setting.
- Fairness metrics can be incompatible and impossible to satisfy simultaneously.

Fairness needs to be defined contextually for the given AI problem.