
Prototype - develop - deploy

both predictive and generative AI

---

- Select pretrained models
- Optimize config settings (eg temperature)
- Refine prompts
- Leverage safe guarding fine tuning

---

2 evaluation methods are offered:

- Computation based: compares against ground truth
- Model based: use LLM as a judge

They are referred to as evaluation pipeline services

2 paradigms:

- pointwise evaluation 
	- evaluate model behaviour in production
	- assess the performance of a single model
- pairwise evaluation
	- determine which model to deploy to production
	- choose between model types
	- select most effective prompts

---

Computation based

Rely on a ground truth datasets.
Fast and affordable, might not catch all nuances
3 metric families:
- lexicon
- count (accuracy) quantify matches
- embeddings (similarity)

VertexAI simplifies the integration of these metrics inside workflows

---

Model based

Mimics human evaluation with LLM judges.

Offer model-based score and evaluation

Example: auto side by side


- data drive objectivity
- offers scalability
- no ground truth

----

Task:
- summarization
- question answering
- tool use
- general text generation

Each task allows a fixed set of metrics

---

- determine evaluation type
- identify task
- identify key aspect
- choose evaluation task and metrics (considering which output is the most important)

----

