
Quantitative measurements of how the model performs on the dataset

---

Dataset is split into train (80) and 10 each for valid and test (these can be customized)

Valid dataset: used to tune the hyperparameters. Doing that on the validation dataset could mean that the model doesn't generalize well

Test set is not used during training, only after.

---

Regression metrics:
- MAE
- MAPE
- RMSE (more sensitive to outliers than mae, so it's preferable if we are interested in higher errors)
- RMSLE: root mean squared logarithmic error. penalizes differences for large prediction values more heavily than for small prediction vales (only if all labels and predicted values are not negative)
- R2

- Feature importance


Classification metrics:

- ROC-AUC: from 0 to 1, higher is better
- PR-AUC
- Log loss
- F1 score (good especially when datasets is unbalanced)
- Precision
- Precision - recall
- Confusion matrix

---

Evaluation can be made through UI, rest api or python library

---

To test a model, endpoints need to be set up

---

Deploy model:
- batch predictions:
	- async
	- many prediction at a time
- online predictions
	- sync
	- one pred requests per API call