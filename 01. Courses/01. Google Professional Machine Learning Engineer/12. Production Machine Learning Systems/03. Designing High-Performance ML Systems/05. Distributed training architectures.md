
Multi core -> GPU to

Multicore + multiple devices GPUS

Multiple machines with multiple devices

Workers work in parallel

---

Types

- Data parallelism

Run the same model and computation on every device, but on different data splits

Each device computes loss gradients and then the model's parameters are updated using these gradients. The updated model is used in the next round of computation

2 approaches to do this:

- synchronous

all devices communicate the locally computed gradients and then exchanges the gradient at the end of each training iteration.
The optimizer performs the parameter updates with these reduced gradients, keeping the devices in sync

- asynchrounous

Parmeter devices -> store variables

Worker devices -> run computations

Each worker independently fetches parameter and computes gradients on a split of the dataset, sending back the gradients

Workers are not waiting for each other, but they can get outdated

---

Async -> great for sparse data
Sync -> good for dense models, like BERT

---

- Model parallelism

Feeds every process the same data but uses different parts of the model

The generated output after each layer needs sto be synchronized to provide the input to the next layer

Needs care when assigning different layers

The gradients obtained from each model and each GPU are accumulated after a backward process, and the parameters are synchronized and updated.

My note: i get this technique makes sense when the model is too big and it has to be split across devices. But if the output of each worker needs to be fed to the one containing the next layer, it's not about going faster


---

Model and data parallelism might be used to

