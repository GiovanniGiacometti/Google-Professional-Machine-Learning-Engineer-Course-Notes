
ML results are not easy to reproduce. The same code might give different results in different contexts

MLflow's solution to this is a self-contained training called Project Specification, that bundles all of the machine learning training code with this version library dependencies, its configurations, and its training and test data.

---

- Code folder
- Define dependencies
- Execution API for running projects

![[Pasted image 20240922200627.png]]

conda file specifies the dependecies that the model depends on

ML project configuration file defines a set of the entry point 
This example has 2 parameters, training data (which can reference external artifacts that exists on a variety of storage solutions) and a float parameter for the model

We can execute the project in those 3 different ways, also specifying a github repository that mlflow will clone for us