
Cloud Composer can be used to orchestrate training pipelines

---

Underlying technology: 

Apache airflow

Author, schedule and monitor workflows.

Workflows are authored as directed acyclic graphs and are configured in python.

Python SDK, established interfaces

Flexible scheduling and dependency management.

Rich library of connectors

Downside:
- setup
- management
- logging

can be time consuming

---

Cloud Composer is a managed Apache Airflow service that helps you create, schedule, monitor and manage workflows

Google Kubernetes Engine cluster with Airflow installed

Logging and monitoring agents are preinstalled in your cluster, so you can use cloud operations to monitor and debug the Cloud Composer environment

Tunnel to authenticate users

---

It integrates with BigQuery, Cloud Storage ...
The infra is fully managed by Google
Workflows are secured using tools such as Cloud IAM

-> Cloud Storage can be used as a source or sink

Same for PubSub for streaming data

Dataflow jobs can be started or Dataproc (if written in spark or hadoop)

BigQuery, AI Platform can be used 

---

It's portable.

Rich library of connectors (usually well built) even to move data from one cloud to another.
