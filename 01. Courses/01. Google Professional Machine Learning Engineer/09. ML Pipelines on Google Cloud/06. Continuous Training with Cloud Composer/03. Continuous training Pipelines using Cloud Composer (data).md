
6 stages of CT pipelines:

- Data Ingestion
- Data validation
- Data preparation (feature engineering)
- Model training
- Model evaluation (how does the new model perform compared to previous model)
- Model deployment 

---

Data Ingestion

- BigQueryToCloudStorage operator 
- Other operators to move data from other clouds or on premise

---

Data Validation

Both to ensure no drifts occurred and still meets expectation, but also to prevent retraining if no new data is available.

- BigQuery check operator for instance is available. This task fails if the query returns a false or a no.
Example:

![[Pasted image 20240922190010.png]]


Notice the string inside brackes: Jinja Template capabilities of Airflow

Basically, we retrieve the rows where the trip start timestamp is greater than 30 days ago, since we don't want to do our routine monthly retraining if no new data are available from that period.


- BigQueryIntervalCheckOperator: check that values are within certain tolerance with respect to values from days_back before (dynamic value)
- BigQueryValueCheckOperator: tolerance with respect to an expected pass value (fixed before execution)

What to do when this task fail? 

TriggerRule.ALL_FAILED to trigger a specific downstream task 

---

Data preparation

- BigQueryOperator
- DataprocCreateClusterOperator + DataprocSubmitJobOperator to do data preprocessing in spark or hadoop
- same for dataflow and beam
