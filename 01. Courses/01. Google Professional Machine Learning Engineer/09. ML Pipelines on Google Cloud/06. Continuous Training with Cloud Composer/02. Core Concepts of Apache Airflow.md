
DAG: collection of tasks that we want to run, organized in a way that reflects their relationships and dependencies (edges of the graph)

A Dag is defined in a Python script -> DAG structure is represented as code

DAG run: physical instances of a DAG, containing task instances

A DAG run is usually created by the Airflow scheduler, but also by an external trigger

Multiple DAG runs for the same DAG can run concurrently, each with a different execution_date

Task: unit of work, implementation of an operator

3 types of operators:
- perform an action
- transfer operators, that move data
- sensors: will keep running until a specific criterion is met

![[Pasted image 20240922183734.png]]

Don't include code outside of DAG code.

Arguments: start date, email, email_on_failure, retries, depends_on_past: defines whether the task should be scheduled according to the previous schedule of the task. 

Instantiation:
DAG as a context manager in python

Pass an id, args, schedule interval and catchup (whether it should backfill the runs with the appropriate execution time for each run)

Tasks:
airflow provides operator for most common tasks, such as a python operator or a bash operator

Also operators for Google Cloud products

Dependencies:

set_upstream and set_downstream (>>, <<)

Can also be used with list

---

Access composer:

- console
- sdk cli
- rest api

---

Finally, how do we get our files onto the Airflow web server?

For DAGs, dependencies, and custom plugins, Cloud Composer performs a periodic one-way R sync between the cloud storage locations listed here and the corresponding local directories.