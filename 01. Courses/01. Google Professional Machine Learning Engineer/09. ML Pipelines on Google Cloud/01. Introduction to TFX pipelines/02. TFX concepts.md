
Components: implementation of a machine learning task in the pipeline.

Designed to be modular.

Each component produce and consume artifacts. 

Components have 5 elements:

- Specification: defines how the component communicates with each other. Specifically:
	- input artifacts
	- output artifacts
	- runtime parameters (required in execution). 
	Specs are implemented as protocol buffers,  which are Google's language neutral, platform neutral extensible mechanism for serializing structured data. Similar to XML or JSON, but smaller, faster and simpler.
- Driver: coordinates job execution
- Executor: code to perform the step
- Publisher: uploads to metadata store
- Interface: packages component specificaiton and executor

---

Runtime:

Driver reads specs for parameters and artifacts and retrieves input artifacts from the metadata store for the component

Executor performs computation on artifacts and generate the output

Publisher uses the specs and store the output artifacts in the metadata store

---

TFX pipeline is a sequence of components connected by channels in a directed acyclic graph (DAG) of artifact dependencies

Parameters:  inputs to pipelines known before the pipeline is executed. They let us change the behaviour of a pipeline instead of changing the components and pipeline code -> great abstraction that allows to increase the pipeline experimentation velocity by running the pipeline in parallel with different sets of parameters.

---

TFX pipelines use ML Metadata for artifact management and an orchestrator for sequential component execution.

It doesn't store the artifacts.

Orchestrator ensure consistency, parallelization.

---

TFX pipelines are both task-aware and data-aware

task-aware: they can be authored in a script or notebook to be run manually

data-aware: store all artifacts from every component over many executions, so they can schedule component runs based on whether artifacts have changed -> this can speed up model retraining. 

---


Horizontal layers coordinate pipeline components -> unless additional customization is needed, we interact only with the front end (no orchestrator, metadata, shared libraries).

4 layers:

- Integrated frontend: GUI based control on pipelines
- Orchestrator: run pipelines based on the DAG
- Shared libraries, such as TFXIO, to control data representation, data access control, data representation
- Pipeline Storage: automatically organize files locally or in a remote file system.

