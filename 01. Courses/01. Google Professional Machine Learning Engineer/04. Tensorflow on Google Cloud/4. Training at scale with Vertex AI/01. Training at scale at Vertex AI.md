
Run distributed TF on Vertex AI

---

Process:
- Package TF as a package (then held in a google storage bucket) 
- submit a training job with gcloud

---

2 general config

- using a pre-built container
- using a custom container

---

single-region bucket is more efficient

---

Jobs can be monitored with the console 

Tensorboard should be used to log information.

---

Vertex AI prediction service can be used to deploy models and scale infrastructure easily