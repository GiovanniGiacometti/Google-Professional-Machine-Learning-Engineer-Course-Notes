
Keras is built-in to TF 2.x

Modular and composable components.
Easy to extend and more user-friendly

---

Sequential allows to stack multiple layers on top of each other

---

Model compilation

We pass the optimizer, the loss function and eventual metrics to monitor.

---

Adam is suited for models that have a large dataset or if there are a lot of parameters to learn.
Other optimizers are momentum, adagrad, adadelta, follow-the-regularizer

These are all good defaults

---

model.fit()

- steps per epoch is the number of batches before finishing an epoch (https://medium.com/google-cloud/ml-design-pattern-3-virtual-epochs-f842296de730 here it explains how to set it:

Setting only epochs is not a good idea: they are integers and don't allow for granularity, forcing to continue training until that number is reached and possibly wasting computational resources. Moreover, checkpoints are execute at each epoch, which might be too much delay. Finally, if more samples are added, the model trained and an higher error is obtained, we can't easily find the cause. The epoch size has increased (the dataset has more samples) but we don't know if the problem is related to the samples being corrupted or an early stopping needed.

A second option is to fix the number of steps, like:

```python
NUM_STEPS = 143000  
BATCH_SIZE = 100  
NUM_CHECKPOINTS = 15
cp_callback = tf.keras.callbacks.ModelCheckpoint(...)  
history = model.fit(trainds,   
                    validation_data=evalds,  
                    epochs=NUM_CHECKPOINTS,  
                    steps_per_epoch=NUM_STEPS // NUM_CHECKPOINTS,   
                    batch_size=BATCH_SIZE,  
                    callbacks=[cp_callback])
```

Now, if new samples are added, the number of steps is fixed, which means the number of samples seen by the model are the same, except part of them are new. If the model doesn't converge, these new data points are the issue.

Here the issue is related to hyperparameter tuning: if we want to change the batch size, for instance halving, we'll train for half the time (because each batch has half the number of samples and each epoch would still have NUM_STEPS // NUMCHECKPOINTS batches trained in each epoch)

The solution is the following: keep the total number of training examples shown to the model constant

```python
NUM_TRAINING_EXAMPLES = 1000 * 1000  
STOP_POINT = 14.3  
TOTAL_TRAINING_EXAMPLES = int(STOP_POINT * NUM_TRAINING_EXAMPLES)  
BATCH_SIZE = 100  
NUM_CHECKPOINTS = 15
steps_per_epoch = (TOTAL_TRAINING_EXAMPLES //   
                   (BATCH_SIZE*NUM_CHECKPOINTS))
cp_callback = tf.keras.callbacks.ModelCheckpoint(...)  
history = model.fit(trainds,   
                    validation_data=evalds,  
                    epochs=NUM_CHECKPOINTS,  
                    steps_per_epoch=steps_per_epoch,   
                    batch_size=BATCH_SIZE,  
                    callbacks=[cp_callback])
```

When more data is received, train with old settings, then increase the number of examples to reflect the new data (still running with the same conditions) and, after having trained the model, change the stop point to reflect the number of times you have to traverse the data to attain convergence (early stopping basically).

The idea is to be able to run the same training process with different parameters so that the same conditions can be replicated when you have different hyperparameters and a different number of samples

)
- callback: functions called during training at specific moments

---

model.predict()

Returns a np array of predictions

- steps: total number of steps before declaring the prediction round finished (i think it's useful when the input dataset is an iterator / tf.data.dataset)

