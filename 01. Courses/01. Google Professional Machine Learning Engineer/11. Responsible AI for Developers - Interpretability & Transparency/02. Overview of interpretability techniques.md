
Linear Regression: just inspect coefficients. The magnitude relates to the importance of each features

Deep Neural Network: generally outperforms linear regression in many tasks, increased complexity makes interpretability less easy -> tradeoff

---

Many interpretability techniques

![[Pasted image 20240928220936.png]]

---

- Intrinsic: apply directly to the model, no additional tools (linear regression, decision tree..)
- Post-hoc: after a model is trained (necessary for more complex model)

---

Intrinsically interpretable models remain among the most popular interpretability methods because they are fairly easy to understand
Some drawbacks however:

- if your model isn't complex enough to accurately describe the underlying system and fit the data well, explanations aren't going to be useful because the models are not useful.
- ML models learn patterns and correlations, not causal structures.
- Model internals and interpretable ML methods tell you about your model, not necessarily your data or the underlying data generation system.


This is how intrinsic explanation look like for these models
![[Pasted image 20240928220708.png]]

---

Post hoc techniques:

- local -> explanations for each data point
- global -> aggregated explanation, more complete map of the prediction space, but might fail the single to explain the single prediction

---

- Model agnostic: apply generic post training method for ANY model (manipulate data)

- Model specific: specialized methods to the model type

---

Major categories:

- Feature based explanation: get importance of each feature
- Concept based explanation: similar to feature but it outputs relative important scores to broader concepts instead of each feature
- Example based: provide data points that our target model thinks is similar to a given query data point

